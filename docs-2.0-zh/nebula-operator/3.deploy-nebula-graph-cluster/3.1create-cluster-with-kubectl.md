# 使用 Kubectl 部署{{nebula.name}}集群

!!! Compatibility "历史版本兼容性"

    1.x 版本的 NebulaGraph Operator 不兼容 3.x 以下版本的{{nebula.name}}。

## 前提条件

- [安装 NebulaGraph Operator](../2.deploy-nebula-operator.md)
- [已创建 StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)

- [已安装 LM 并加载 License Key](3.0.deploy-lm.md)


## 创建集群

本文以创建名为`nebula`的集群为例，说明如何部署{{nebula.name}}集群。

1. 创建命名空间，例如`nebula`。如果不指定命名空间，默认使用`default`命名空间。

  ```bash
  kubectl create namespace nebula
  ```

  
2. 创建 Secret，用于拉取私有仓库中{{nebula.name}}镜像。

  ```bash
  kubectl -n <nebula> create secret docker-registry <image-pull-secret> \
  --docker-server=DOCKER_REGISTRY_SERVER \
  --docker-username=DOCKER_USER \
  --docker-password=DOCKER_PASSWORD
  ```

  - `<nebula>`：存放该 Secret 的命名空间。
  - `<image-pull-secret>`：指定 Secret 的名称。
  - `DOCKER_REGISTRY_SERVE`：指定拉取镜像的私有仓库服务器地址，例如`reg.example-inc.com`。
  - `DOCKER_USE`：镜像仓库用户名。
  - `DOCKER_PASSWORD`：镜像仓库密码。
  

3. 创建集群配置文件。

  ??? info "展开查看集群的示例配置"

      ```yaml
      apiVersion: apps.nebula-graph.io/v1alpha1
      kind: NebulaCluster
      metadata:
        name: nebula
        namespace: default
      spec:
        unsatisfiableAction: ScheduleAnyway
        # 是否回收 PVC。
        enablePVReclaim: true
        graphd:
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 19669
              scheme: HTTP
            initialDelaySeconds: 40
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          # Graph 服务的容器镜像。
          image: reg.example-inc.com/xxx/xxx
          logVolumeClaim:
            resources:
              requests:
                storage: 2Gi
            # 用于存储 Graph 服务的日志的存储类名称。
            storageClassName: local-sc
          replicas: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 500Mi
          version: v{{nebula.release}}
        imagePullPolicy: Always
        # 用于从私有仓库拉取镜像的 Secret。
        imagePullSecrets:
        - name: secret-name
        metad:
          # LM 访问地址和端口号，用于获取 License 信息。
          licenseManagerURL: 192.168.x.xxx:9119
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 19559
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          # Meta 服务的容器镜像。
          image: reg.example-inc.com/xxx/xxx
          logVolumeClaim:
            resources:
              requests:
                storage: 2Gi
            storageClassName: local-sc
          dataVolumeClaim:
            resources:
              requests:
                storage: 2Gi
            storageClassName: local-sc
          replicas: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 500Mi
          version: v{{nebula.release}}
        nodeSelector:
          nebula: cloud
        reference:
          name: statefulsets.apps
          version: v1
        schedulerName: default-scheduler
        storaged:
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 19779
              scheme: HTTP
            initialDelaySeconds: 40
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          # Storage 服务的容器镜像。
          image: reg.example-inc.com/xxx/xxx
          logVolumeClaim:
            resources:
              requests:
                storage: 2Gi
            storageClassName: local-sc
          dataVolumeClaims:
          - resources:
              requests:
                storage: 2Gi
            storageClassName: local-sc
          replicas: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 500Mi
          version: v{{nebula.release}}
      ```  
   
  必须自定义修改以下参数，其他参数可按需修改。

  - `spec.metad.licenseManagerURL`
  - `spec.<graphd|metad|storaged>.image`
  - `spec.imagePullSecrets` 

  可配置的参数描述如下：

  | 参数    | 默认值  | 描述    |
  | :---- | :--- | :--- |
  | `metadata.name`              | -                                                            | 创建的{{nebula.name}}集群名称。 |
  |`spec.console`|-| 启动 Console 容器用于连接 Graph 服务。配置详情，参见 [nebula-console](https://github.com/vesoft-inc/nebula-operator/blob/v{{operator.release}}/doc/user/nebula_console.md#nebula-console).|
  | `spec.graphd.replicas`       | `1`                                                          | Graphd 服务的副本数。         |
  | `spec.graphd.image`         | `vesoft/nebula-graphd`                                       | Graphd 服务的容器镜像。       |
  | `spec.graphd.version`        | `{{nebula.tag}}`                                                     | Graphd 服务的版本号。         |
  | `spec.graphd.service`        |                                                             | 访问 Graphd 服务的 Service 配置。      | 
  | `spec.graphd.logVolumeClaim.storageClassName`   | -                                                            | Graphd 服务的日志盘存储卷的存储类名称。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。        |
  | `spec.metad.replicas`        | `1`                                                          | Metad 服务的副本数。          |
  | `spec.metad.image`          | `vesoft/nebula-metad`                                        | Metad 服务的容器镜像。        |
  | `spec.metad.version`         | `{{nebula.tag}}`                                                     | Metad 服务的版本号。          |
  | `spec.metad.dataVolumeClaim.storageClassName`    | -                                                            | Metad 服务的数据盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。        |
  | `spec.metad.logVolumeClaim.storageClassName`|-|Metad 服务的日志盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。 |
  | `spec.storaged.replicas`     | `3`                                                          | Storaged 服务的副本数。       |
  | `spec.storaged.image`       | `vesoft/nebula-storaged`                                     | Storaged 服务的容器镜像。     |
  | `spec.storaged.version`      | `{{nebula.tag}}`                                                     | Storaged 服务的版本号。       |
  | `spec.storaged.dataVolumeClaims.resources.requests.storage` | -                                                            | Storaged 服务的数据盘存储大小，可指定多块数据盘存储数据。当指定多块数据盘时，路径为：`/usr/local/nebula/data1`、`/usr/local/nebula/data2`等。 |
  | `spec.storaged.dataVolumeClaims.resources.storageClassName` | -                                                            | Storaged 服务的数据盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。  |
  | `spec.storaged.logVolumeClaim.storageClassName`|-|Storaged 服务的日志盘存储配置。使用示例配置时需要将其替换为事先创建的存储类名称，参见 [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) 查看创建存储类详情。 |
  |`spec.<metad|storaged|graphd>.securityContext`|`{}`|定义集群容器的权限和访问控制，以控制访问和执行容器的操作。详情参见 [SecurityContext](https://github.com/vesoft-inc/nebula-operator/blob/{{operator.branch}}/doc/user/security_context.md)。 |
  |`spec.agent`|`{}`| Agent 服务的配置。用于备份和恢复及日志清理功能，如果不自定义该配置，将使用默认配置。|
  | `spec.reference.name`        | -                                                            | 依赖的控制器名称。           |
  | `spec.schedulerName`         | -                                                            | 调度器名称。                 |
  | `spec.imagePullPolicy`       | {{nebula.name}}镜像的拉取策略。关于拉取策略详情，请参考 [Image pull policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy)。 | 镜像拉取策略。               |
  |`spec.logRotate`| - |日志轮转配置。详情参见[管理集群日志](../8.custom-cluster-configurations/8.4.manage-running-logs.md)。|
  |`spec.enablePVReclaim`|`false`|定义是否在删除集群后自动删除 PVC 以释放数据。详情参见[回收 PV](../8.custom-cluster-configurations/storage/8.2.pv-reclaim.md)。|
  | `spec.metad.licenseManagerURL`       | - | 配置指向 [LM](../../9.about-license/2.license-management-suite/3.license-manager.md) 的 URL，由 LM 的访问地址和端口（默认端口`9119`）组成。例如，`192.168.8.100:9119`。**必须配置此参数以获取 License 信息，否则无法使用{{nebula.name}}集群。** |
  |`spec.storaged.enableAutoBalance`| `false`| 是否启用自动均衡。详情参见[均衡扩容后的 Storage 数据](../8.custom-cluster-configurations/storage/8.3.balance-data-when-scaling-storage.md)。|
  |`spec.enableBR`|`false`|定义是否启用 BR 工具。详情参见[备份与恢复](../10.backup-restore-using-operator.md)。|
  |`spec.imagePullSecrets`| - |定义拉取私有仓库中镜像所需的 Secret。|


4. 创建{{nebula.name}}集群。

  ```bash
  kubectl create -f apps_v1alpha1_nebulacluster.yaml
  ```

  返回：

  ```bash
  nebulacluster.apps.nebula-graph.io/nebula created
  ```


5. 查看{{nebula.name}}集群状态。
   
  ```bash
  kubectl get nebulaclusters nebula
  ```

  返回：

  ```bash
  NAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE
  nebula   1                1              1               1             1                  1                86s
  ```

## 创建带 Zone 的集群

NebulaGraph 利用 Zone 的功能来高效管理其分布式架构。每个 Zone 代表了存储完整图空间数据的 Storage Pod 的逻辑分组。NebulaGraph 的图空间数据被切分为不同分片，并且这些分片的副本均匀分布在所有可用的 Zone 中，同时查询可以优先发送到同一 Zone 内的 Storage Pod 中。使用 Zone 可显著降低 Zone 间的网络流量成本并提高数据传输速度。关于 Zone 的详细信息，请参见[管理 Zone](../../4.deployment-and-installation/5.zone.md)。

??? info "展开查看带 Zone 集群的示例配置"

    ```yaml
    apiVersion: apps.nebula-graph.io/v1alpha1
    kind: NebulaCluster
    metadata:
      name: nebula
      namespace: default
    spec:
      # 用于获取节点所在位置的 Zone 信息。
      alpineImage: "reg.example-inc.com/xxx/xxx:latest"
      # 用于备份和恢复以及日志清理功能。
      # 如果你不自定义此配置，将使用默认配置。
      agent:
        image: reg.example-inc.com/xxx/xxx
        version: v{{nebula.release}}
      exporter:
        image: vesoft/nebula-stats-exporter
        replicas: 1
        maxRequests: 20
      # 用于创建一个控制台容器，用于连接到集群。
      console:
        version: "nightly"
      graphd:
        config:
          # 以下参数是创建具有 Zone 的集群所必需的。
          accept_partial_success: "true"
          prioritize_intra_zone_reading: "true"
          sync_meta_when_use_space: "true"
          stick_to_intra_zone_on_failure: "false" 
          session_reclaim_interval_secs: "300"
          # 以下参数是收集日志所必需的。
          logtostderr: "1"
          redirect_stdout: "false"
          stderrthreshold: "0" 
        resources:
          requests:
            cpu: "2"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "2Gi"
        replicas: 1
        image: reg.example-inc.com/xxx/xxx
        version: v3.5.0-sc
      metad:
        config:
          redirect_stdout: "false"
          stderrthreshold: "0"
          logtostder: "true"
          # Zone 名称一旦设置就不能修改。
          # 建议设置奇数个 Zone。
          zone_list: az1,az2,az3 
          validate_session_timestamp: "false"
        # LM 访问地址和端口号。
        licenseManagerURL: "192.168.8.xxx:9119"
        resources:
          requests:
            cpu: "300m"
            memory: "500Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
        replicas: 3
        image: reg.example-inc.com/xxx/xxx
        version: v3.5.0-sc
        dataVolumeClaim:
          resources:
            requests:
              storage: 2Gi
          storageClassName: local-path
      storaged:
        config:
          redirect_stdout: "false"
          stderrthreshold: "0"
          logtostder: "true"
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "2Gi"
        replicas: 3
        image: reg.example-inc.com/xxx/xxx
        version: v3.5.0-sc
        dataVolumeClaims:
        - resources:
            requests:
              storage: 2Gi
          storageClassName: local-path
        # 扩容后自动平衡存储数据。
        enableAutoBalance: true
      reference:
        name: statefulsets.apps
        version: v1
      schedulerName: nebula-scheduler
      nodeSelector:
        nebula: cloud
      imagePullPolicy: Always
      imagePullSecrets:
      - name: nebula-image
      # 在 Zone 之间均匀分布存储 Pods。
      # 使用 Zone 时必须设置。
      topologySpreadConstraints:
      - topologyKey: "topology.kubernetes.io/zone"
        whenUnsatisfiable: "DoNotSchedule"
    ```

### 配置 Zone

如要充分利用 Zone 功能，首先需要确定集群节点所在的实际 Zone。通常情况下，部署在云平台上的节点都带有各自 Zone 的标签。一旦获取了这些信息，可以通过在集群的配置文件中设置`spec.metad.config.zone_list`参数来进行配置。该参数是一个由逗号分隔的 Zone 名称列表，并且应与节点实际所在的 Zone 名称相匹配。例如，如果用户节点实际位于 az1、az2 和 az3 区域，配置应如下所示：

```yaml
spec:
  metad:
    config:
      zone_list: az1, az2, az3
```

### 使用 Zone

NebulaGraph Operator 利用 Kubernetes 的 TopologySpread 功能来管理 Storage Pod 和 Graph Pod 的调度。一旦配置了`zone_list`，Storage Pod 将根据`topology.kubernetes.io/zone`标签自动分配到各自的 Zone 中。

对于 Zone 内数据访问，Graph Pod 会使用`--assigned_zone=$NODE_ZONE`参数动态分配自己到一个 Zone 内。通过一个 init-container 来获取节点上的 Graph Pod 所在 Zone 的名称。在`spec.alpineImage`（默认值：`reg.example-inc.com/nebula-alpine:latest`）中指定的 Alpine Linux 镜像可获取 Zone 信息。

### 优先访问同 Zone 数据

通过在集群配置文件中将`spec.graphd.config.prioritize_intra_zone_reading`设置为`true`，可以使 Graph Pod 优先将查询发送到同一 Zone 内的 Storage Pod。在该 Zone 内读取失败的情况下，行为取决于`spec.graphd.config.stick_to_intra_zone_on_failure`的值。如果设置为`true`，Graph 服务将避免从其他 Zone 内读取数据并返回错误。否则，它将读取其他 Zone 内分片的 Leader 副本数据。

```yaml
spec:
  alpineImage: reg.example-inc.com/xxx/xxx:latest
  graphd:
    config:
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false"
```

### 必配参数

如果需要创建带 Zone 的集群，必需在集群配置文件中添加以下字段。其他可选字段描述见上文**创建集群**。

```yaml
spec:
  # 用来获取节点所在的 Zone 信息。
  alpineImage: "reg.example-inc.com/xxx/xxx:latest"
  graphd:
    image: reg.example-inc.com/xxx/xxx
    config:
      # 优先将查询发送到同一 Zone 内的存储节点。
      prioritize_intra_zone_reading: "true"
      stick_to_intra_zone_on_failure: "false" 
  metad:
    image: reg.example-inc.com/xxx/xxx
    config:
      # Zone 的名称列表，由逗号分隔。建议设置为奇数。
      zone_list: az1,az2,az3 
    licenseManagerURL: "192.168.8.xxx:9119"
  storaged:
    image: reg.example-inc.com/xxx/xxx
  imagePullSecrets:
  - name: nebula-image
  # 用于调度重启的 Graph/Storage Pods 至原来的 Zone。
  schedulerName: nebula-scheduler
  # 用于控制存储 Pod 分布的字段。
  topologySpreadConstraints:
  - topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "DoNotSchedule"
```

参数描述如下：

| 参数                             | 默认值                 | 描述                                                                                                       |
| ------------------------------ | -------------------- | -------------------------------------------------------------------------------------------------------- |
| `spec.metad.licenseManagerURL` | -                    | 配置指向许可证管理器（LM）的 URL，其中包括 LM 的访问地址和端口号（默认端口 `9119`）。例如，`192.168.8.xxx:9119`。**必须配置此参数以获取许可证信息；否则，无法使用企业版集群。** |
| `spec.<graphd|metad|storaged>.image`      | -                    | 企业版 Graph、Meta 或 Storage 服务的容器镜像。                                                                         |
| `spec.imagePullSecrets`        | -                    | 用于从私有仓库拉取 NebulaGraph 企业版服务镜像的 Secret。                                                                     |
| `spec.alpineImage`             | -                    | 用于获取节点所在 Zone 信息的 Alpine Linux 镜像。                                                                         |
| `spec.metad.config.zone_list`  | -                    | Zone 的名称列表，由逗号分隔。建议设置奇数个 Zone。例如：zone1,zone2,zone3。<br/>**一旦设置， Zone 名称无法更改。**                             |
| `spec.graphd.config.prioritize_intra_zone_reading` | `false` | 指定是否优先将查询发送到同一 Zone 内的存储节点。<br/>当设置为`true`时，查询将发送到同一 Zone 的存储节点。如果在该区 Zone 读取失败，它将根据`stick_to_intra_zone_on_failure`决定是否读数据分片的 Leader 副本数据。          |
| `spec.graphd.config.stick_to_intra_zone_on_failure` | `false` | 当设置为`false`时，如果在同一 Zone 内没有找到所请求数据时，将读取分片的 Leader 副本数据。当设置为`true`时，如果在该 Zone 内未读取到分片数据，将返回报错。|
| `schedulerName`| - | 调度器名称。使用 Zone 功能时，必须配置为`nebula-scheduler`，用于调度重启的 Graph 和 Storage Pods 至原始的 Zone。 |
| `spec.topologySpreadConstraints`              | -                    | 这是 Kubernetes 中用于控制 Storage Pod 分布的字段。其目的是确保其均匀分布在各个 Zone 中。<br/>**要使用 Zone 功能，必须将`topologySpreadConstraints[0].topologyKey`的值设置为`topology.kubernetes.io/zone`，并将`topologySpreadConstraints[0].whenUnsatisfiable`的值设置为`DoNotSchedule`**。运行`kubectl get node --show-labels`来检查该键信息。有关更多信息，请参阅[TopologySpread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#example-multiple-topologyspreadconstraints)。 |

!!! warning

    请勿手动修改 NebulaGraph Operator 创建的 ConfigMaps，否则可能会导致意外的行为。
    
    在 Storage Pod 和 Graph Pod 被分配到了各个 Zone 后，与其对应的 Zone 之间的映射信息将存储在名为`<cluster_name>-graphd|storaged-zone`的 ConfigMap 中。这个映射在滚动更新和 Pod 重启期间有助于 Pod 的调度，确保服务返回到所需的原始 Zone 中。

!!! caution

    创建集群后，导入数据前，确保 Storage Pod 均匀分布在各个 Zone 中以提高集群的韧性。运行`SHOW ZONES`命令来检查 Storage Pod 是否均匀分布在各个 Zone 中。有关 Zone 相关命令，请参阅[管理 Zone](../../4.deployment-and-installation/5.zone.md)。


## 扩缩容集群


仅支持通过 v1.1.0 及以上版本的 NebulaGraph Operator 扩缩容{{nebula.name}}集群。
  
用户可以通过编辑`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值进行{{nebula.name}}集群的扩缩容。

### 扩容集群

本文举例扩容{{nebula.name}}集群中 Storage 服务至 5 个。步骤如下：

1. 将`apps_v1alpha1_nebulacluster.yaml`文件中`storaged.replicas`的参数值从`3`改为`5`。

  ```yaml
    storaged:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 5
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaims:
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      - resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
  ```

2. 执行以下命令使上述更新同步至{{nebula.name}}集群 CR 中。

  ```bash
  kubectl apply -f apps_v1alpha1_nebulacluster.yaml
  ```
  
3. 查看 Storage 服务的副本数。

  ```bash
  kubectl get pods -l app.kubernetes.io/cluster=nebula
  ```
  返回：

  ```bash
  NAME                READY   STATUS    RESTARTS   AGE
  nebula-graphd-0     1/1     Running   0          2m
  nebula-metad-0      1/1     Running   0          2m
  nebula-storaged-0   1/1     Running   0          2m
  nebula-storaged-1   1/1     Running   0          2m
  nebula-storaged-2   1/1     Running   0          2m
  nebula-storaged-3   1/1     Running   0          5m
  nebula-storaged-4   1/1     Running   0          5m
  ```
  由上可看出 Storage 服务的副本数被扩容至 5 个。

### 缩容集群

缩容集群的原理和扩容一样，用户只需将`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值缩小。具体操作，请参考上文的**扩容集群**部分。

如果缩容操作长时间处于未完成状态，可以进入通过`spec.console`字段启动的 Console 容器，然后查看缩容 Job 状态。如果缩容 Job 处于`FAILED`状态，可以查看 Meta 服务的日志，找到导致缩容失败的原因。关于更多 Job 的信息，参见[作业管理](../../3.ngql-guide/4.job-statements.md)。

!!! caution

    - 目前仅支持对{{nebula.name}}集群中的 Graph 服务和 Storage 服务进行扩缩容，不支持扩缩容 Meta 服务。
    - 如果缩容带 Zone 功能的集群，需要确保缩容后的剩余的 Storage Pod 数量不少于通过`spec.metad.config.zone_list`指定的 Zone 的数量。例如，如果 Zone 的数量为 3，那么缩容后的 Storage Pod 数量不能少于 3 个。

### 启用 Pod 水平自动扩缩

NebulaGraph Operator 支持使用 NebulaAutoscaler 对象为 Graph 服务配置 [Pod 水平自动扩缩](https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/) (Horizontal Pod Autoscaling，下文简称为 HPA)，实现基于资源使用情况自动调整 Pod 数量。

下文将介绍在{{nebula.name}}集群启用 HPA 的流程。

1. 安装 metrics-server。

  NebulaAutoscaler 基于 K8s 的 HorizontalPodAutoscaler 设计，依赖于 metrics-server 收集的资源指标数据从而实现自动调整 Pod 数量。

  执行以下命令安装最新版本的 metrics-server。
  
  ```bash
  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
  ```

2. 验证 metrics-server 运行正常。

  metrics-server 暴露了 Metrics API，用来提供关于节点和 Pods 资源使用情况的信息。下面将通过访问 Metrics API 来获取 metrics-server 收集到的资源使用量数据。若成功返回资源使用量数据，则说明 metrics-server 运行正常。

  执行以下命令查看名为`nebula-graphd-1`的 Pod 的资源使用情况：

  ```bash
  kubectl get --raw "/apis/metrics.k8s.io/v1beta1/namespaces/default/pods/nebula-graphd-1" | jq '.'
  ```

  返回示例：

  ```json
  {
    "kind": "PodMetrics",
    "apiVersion": "metrics.k8s.io/v1beta1",
    "metadata": {
      "name": "nebula-graphd-1",
      "namespace": "default",
      "creationTimestamp": "2023-09-27T13:39:54Z",
      "labels": {
        "app.kubernetes.io/cluster": "nebula",
        "app.kubernetes.io/component": "graphd",
        "app.kubernetes.io/managed-by": "nebula-operator",
        "app.kubernetes.io/name": "nebula-graph",
        "controller-revision-hash": "nebula-graphd-56cf5f8b66",
        "statefulset.kubernetes.io/pod-name": "nebula-graphd-1"
      }
    },
    "timestamp": "2023-09-27T13:39:48Z",
    "window": "15.015s",
    "containers": [
      {
        "name": "graphd",
        "usage": {
          "cpu": "323307n",
          "memory": "12644Ki"
        }
      }
    ]
  }
  ```

  执行以下命令查看名为`192-168-8-35`的节点的资源使用情况：

  ```bash
  kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes/192-168-8-35" | jq '.'
  ```

  返回示例：

  ```json
  {
    "kind": "NodeMetrics",
    "apiVersion": "metrics.k8s.io/v1beta1",
    "metadata": {
      "name": "192-168-8-35",
      "creationTimestamp": "2023-09-27T14:00:13Z",
      "labels": {
        "beta.kubernetes.io/arch": "amd64",
        "beta.kubernetes.io/os": "linux",
        "kubernetes.io/arch": "amd64",
        "kubernetes.io/hostname": "192-168-8-35",
        "kubernetes.io/os": "linux",
        "nebula": "cloud",
        "node-role.kubernetes.io/control-plane": "",
        "node.kubernetes.io/exclude-from-external-load-balancers": ""
      }
    },
    "timestamp": "2023-09-27T14:00:00Z",
    "window": "20.045s",
    "usage": {
      "cpu": "164625163n",
      "memory": "8616740Ki"
    }
  }
  ```

3. 创建 NebulaAutoscaler 对象。

  以下示例定义的 NebulaAutoscaler 对象会基于平均 CPU 使用率，将 Pods 数量维持在 2 到 5 个之间。

  ```yaml
  apiVersion: autoscaling.nebula-graph.io/v1alpha1
  kind: NebulaAutoscaler
  metadata:
    name: nebula-autoscaler
  spec:
    nebulaClusterRef:
      name: nebula
    graphdPolicy:
      minReplicas: 2
      maxReplicas: 5
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 50
    pollingPeriod: 30s
  ```

  关键参数的描述如下:

  - nebulaClusterRef：该 NebulaAutoscaler 作用的目标集群。
  - graphdPolicy：该 NebulaAutoscaler 采用的伸缩策略。该参数的子字段与 K8s 的 HorizontalPodAutoscaler 支持的字段完全兼容。详细字段定义请参考 [API 文档](https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec)。 
  - pollingPeriod: NebulaAutoscaler 进行资源使用率检查的时间间隔。

  此外，NebulaAutoscaler 也支持`behavior`参数，用于定义 Pod 数量扩张和收缩时的不同表现，实现更精细化的伸缩控制。

  在使用`behavior`参数前，建议先完全理解其 [API 文档](https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec)中的字段定义。

  以下示例定义的 NebulaAutoscaler 会在 Pod 数量扩张和收缩时采取不同的行为：

  ```yaml
  apiVersion: autoscaling.nebula-graph.io/v1alpha1
  kind: NebulaAutoscaler
  metadata:
    name: nebula-autoscaler
  spec:
    nebulaClusterRef:
      name: nebula
    graphdPolicy:
      minReplicas: 2
      maxReplicas: 5
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 50
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
          - type: Pods
            value: 4
            periodSeconds: 15
          selectPolicy: Max
    pollingPeriod: 30s
  ```

4. 验证 HPA 效果。

  在执行`kubectl apply`命令创建 NebulaAutoscaler 对象后，可以使用以下方式检查 HPA 的效果。

  执行`kubectl get na`命令检查 NebulaAutoscaler 的配置和状态。

  返回示例：

  ```
  NAME                REFERENCE   MIN-REPLICAS   MAX-REPLICAS   CURRENT-REPLICAS   ACTIVE   ABLETOSCALE   LIMITED   READY   AGE
  nebula-autoscaler   nebula      2              5              2                  True     True          True      True    19h
  ```

  执行`kubectl get nc`命令检查{{nebula.name}}集群的状态。

  返回示例：

  ```
  NAME     READY   GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE
  nebula   True    2                2              1               1             3                  3                20h
  ```

## 删除集群

使用 Kubectl 删除{{nebula.name}}集群的命令如下：

```bash
kubectl delete -f apps_v1alpha1_nebulacluster.yaml
```

## 后续操作

[连接{{nebula.name}}数据库](../4.connect-to-nebula-graph-service.md)

